# Data Pipelines - Complete Guide

Generic, reusable pipelines for detection and classification datasets.

---

## ğŸš€ Quick Start

### Detection Pipeline
```bash
python -m data_pipelines.cli_detection \
    --base-dir ./my_project \
    --api-url http://dev.api.domain-name.ai/dataset/api/v1/dataset-versions/detail \
    --dataset-version-ids 42 \
    --step full
```

### Classification Pipeline
```bash
python -m data_pipelines.cli_classification \
    --base-dir ./my_project \
    --api-url http://dev.api.domain-name.ai/dataset/api/v1/dataset-versions/detail \
    --dataset-version-ids 42 \
    --step full
```

### Test First (Dry Run)
```bash
# Add --dry-run to any command
python -m data_pipelines.cli_detection --base-dir . --api-url ... --dataset-version-ids 42 --step full --dry-run
```

---

## ğŸ“ Output Structure

```
your_project/
â”œâ”€â”€ client_data/              # Downloaded from API
â”‚   â”œâ”€â”€ positive_samples/
â”‚   â”‚   â”œâ”€â”€ adenoma/
â”‚   â”‚   â”œâ”€â”€ hyperplastic/
â”‚   â”‚   â””â”€â”€ benign/
â”‚   â””â”€â”€ negative_samples/
â”œâ”€â”€ detection_dataset/        # For detection (binary masks)
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ masks/
â”‚   â””â”€â”€ negative_samples/
â”œâ”€â”€ classification_dataset/   # For classification (colored masks)
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ masks/
â”‚   â””â”€â”€ negative_samples/
â”œâ”€â”€ coco/                     # Detection COCO format
â””â”€â”€ coco_classification/      # Classification COCO format
```

---

## ğŸ”§ Pipeline Steps

### Step 1: Download
Downloads data from API with S3 integration.

```bash
python -m data_pipelines.cli_detection \
    --step download \
    --api-url http://dev.api.domain-name.ai/dataset/api/v1/dataset-versions/detail \
    --dataset-version-ids 42
```

### Step 2: Organize

**Detection**: Aggregates all classes into single folders
```bash
python -m data_pipelines.cli_detection --step organize --base-dir ./my_project
```

**Classification**: Converts masks to colored format with class-specific colors
```bash
python -m data_pipelines.cli_classification --step organize --base-dir ./my_project
```

### Step 3: Clean
Removes unmatched/corrupted files
```bash
python -m data_pipelines.cli_detection --step clean --base-dir ./my_project
```

### Step 4: Prepare
Converts to COCO format
```bash
python -m data_pipelines.cli_detection --step prepare --base-dir ./my_project
```

---

## ğŸ¯ Common Scenarios

### Already Downloaded Data
```bash
python -m data_pipelines.cli_detection --base-dir . --step full --skip-download
```

### Multiple Dataset Versions
```bash
python -m data_pipelines.cli_detection \
    --base-dir ./combined \
    --api-url http://dev.api.domain-name.ai/dataset/api/v1/dataset-versions/detail \
    --dataset-version-ids 40 41 42 43 \
    --step full
```

### Re-organize Existing Data
```bash
python -m data_pipelines.cli_detection --base-dir . --step organize
```

---

## ğŸ Python API

```python
from pathlib import Path
from data_pipelines.pipelines import DetectionPipeline

# Initialize
pipeline = DetectionPipeline(
    base_dir=Path("./my_project"),
    dataset_version_ids=[42],
    api_url="http://dev.api.domain-name.ai/dataset/api/v1/dataset-versions/detail"
)

# Run full pipeline
results = pipeline.run_full_pipeline()

# Or run individual steps
pipeline.run_download()
pipeline.run_organize()
pipeline.run_clean()
pipeline.run_prepare()
```

---

## ğŸ“Š Data Flow

### Detection Pipeline
```
API â†’ client_data/ â†’ detection_dataset/ â†’ coco/
      (download)     (organize)           (prepare)
                     (clean)
```

**Key Logic**:
- Binary masks (any non-black pixel = foreground)
- Connected components â†’ Bounding boxes
- Single category: "polyp"
- 80/20 train/val split

### Classification Pipeline
```
API â†’ client_data/ â†’ classification_dataset/ â†’ coco_classification/
      (download)     (organize + color)        (prepare)
                     (clean)
```

**Key Logic**:
- Colored masks:
  - Adenoma: Red (255, 0, 0)
  - Hyperplastic: Green (0, 255, 0)
  - Benign: Purple (157, 0, 255)
  - No Pathology: White (255, 255, 255)
- Multi-class bounding boxes
- 80/20 train/val split

---

## ğŸ—ï¸ Architecture

```
data_pipelines/
â”œâ”€â”€ core/              # Shared utilities
â”‚   â”œâ”€â”€ config.py      # Configuration classes
â”‚   â”œâ”€â”€ file_utils.py  # File operations
â”‚   â”œâ”€â”€ s3_utils.py    # S3 operations
â”‚   â””â”€â”€ logger.py      # Logging
â”œâ”€â”€ downloaders/       # Data acquisition
â”‚   â”œâ”€â”€ api_downloader.py
â”‚   â””â”€â”€ s3_downloader.py
â”œâ”€â”€ organizers/        # Dataset organization
â”‚   â”œâ”€â”€ detection_organizer.py
â”‚   â””â”€â”€ classification_organizer.py
â”œâ”€â”€ cleaners/          # Dataset cleaning
â”‚   â””â”€â”€ dataset_cleaner.py
â”œâ”€â”€ preparers/         # COCO format
â”‚   â”œâ”€â”€ detection_preparer.py
â”‚   â””â”€â”€ classification_preparer.py
â””â”€â”€ pipelines/         # End-to-end workflows
    â”œâ”€â”€ detection_pipeline.py
    â””â”€â”€ classification_pipeline.py
```

---

## âš™ï¸ Configuration

All components use configuration classes:

```python
from data_pipelines.core import PreparerConfig

config = PreparerConfig(
    input_dir=Path("./detection_dataset"),
    output_dir=Path("./coco"),
    train_split=0.85,
    min_area_threshold=100,
    add_negative_samples=True,
    seed=42
)
```

---

## ğŸ”Œ Extending the Pipeline

### Add Custom Downloader
```python
from data_pipelines.downloaders import BaseDownloader

class MyDownloader(BaseDownloader):
    def download(self):
        # Your logic
        pass
```

### Add Custom Organizer
```python
from data_pipelines.organizers import BaseOrganizer

class MyOrganizer(BaseOrganizer):
    def organize(self):
        # Your logic
        pass
```

---

## ğŸ“ Migration from Old Scripts

| Old Script | New Command |
|------------|-------------|
| `dataset_versions_downloader.py` | `--step download` |
| `prepare_dataset.py` | `--step prepare` (detection) |
| `prepare_dataset_classification.py` | `--step prepare` (classification) |
| `clean_and_organize_classification_dataset.py` | `--step organize` (classification) |
| `clean_up_dataset.py` | `--step clean` |

**One command replaces all**:
```bash
python -m data_pipelines.cli_detection --step full
```

---

## âš ï¸ Important Notes

1. **Always use `-m` flag**: `python -m data_pipelines.cli_detection`
2. **Test with `--dry-run`** before running on production data
3. **Module name**: `data_pipelines` (underscore, not hyphen)
4. **Path structure**: Files go to `base_dir/client_data/` (simplified)

---

## ğŸ› Troubleshooting

### "client_data directory not found"
Run download step first or check `--base-dir` path.

### "No module named 'data_pipelines'"
Use `python -m data_pipelines.cli_detection` (with `-m` flag).

### "No matching mask for image"
Check that image and mask filenames match (same stem, different extensions).

---

## ğŸ“š Additional Documentation

- **MIGRATION_GUIDE.md** - Detailed migration from old scripts
- **BUGFIXES.md** - Technical details of fixes applied
- **WORKFLOW_DIAGRAM.md** - Visual diagrams

---

## âœ… Features

- âœ… Modular architecture
- âœ… Type-safe configurations
- âœ… Dry run support
- âœ… Parallel downloads
- âœ… Automatic validation
- âœ… Progress bars
- âœ… Comprehensive logging
- âœ… Error recovery with retries

---

## ğŸ“¦ Requirements

```bash
pip install boto3 opencv-python numpy Pillow tqdm requests
```

---

## ğŸ‰ Ready to Use!

```bash
# Start here
python -m data_pipelines.cli_detection \
    --base-dir . \
    --api-url http://<dev.api.domain-name.ai>/dataset/api/v1/dataset-versions/detail \
    --dataset-version-ids 42 \
    --step full \
    --dry-run
```

Then remove `--dry-run` to run for real!
